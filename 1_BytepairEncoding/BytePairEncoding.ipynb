{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8b5b9962939f84d7d7e162d6e314010",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    Assignment 01\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Monday at 13:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to two people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-biography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.324137Z",
     "start_time": "2024-04-20T00:37:44.303265Z"
    },
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the username of each team member into the variables. \n",
    "If you work alone please leave the second variable empty.\n",
    "'''\n",
    "member1 = 'mfarra2s'\n",
    "member2 = ''\n",
    "member3 = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b44acb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a71fd5a7ffb0b58b00993e75290a656",
     "grade": false,
     "grade_id": "BytePairEncoding_ABytePairEncoding_BBytePairEncoding_CBytePairEncoding_DBytePairEncoding_EBytePairEncoding_FBytePairEncoding_G_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Byte Pair Encoding\n",
    "\n",
    "We want to implement BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a834fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0944bf41e5684886879b9a5f916a1712",
     "grade": false,
     "grade_id": "BytePairEncoding_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding A) [10 points]\n",
    "\n",
    "First we want to do pre-tokenization using white spaces.\n",
    "\n",
    "Please complete the function `pretokenize` below. This takes a list of sentences or documents and returns a list of tokenized sentences or documents. Look at the example in the docstring for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab080389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.414361Z",
     "start_time": "2024-04-20T00:37:44.328622Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0e475e7b6ed8c011d3984b8b8e3787c",
     "grade": false,
     "grade_id": "BytePairEncoding_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'an', 'example', 'sentence'],\n",
       " ['Another', 'sentence'],\n",
       " ['The', 'final', 'sentence']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def pretokenize(sentences: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes a list of sentences into a list of lists of tokens.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): List of sentences to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        List[List[str]]: List of lists of tokens, where each inner list represents\n",
    "                         the tokens of a single sentence.\n",
    "    Example:\n",
    "        >>> sentences = [\"Hello world\", \"This is a test\"]\n",
    "        >>> pretokenize(sentences)\n",
    "        [['Hello', 'world'], ['This', 'is', 'a', 'test']]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = sentence.split()\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "#     raise NotImplementedError()\n",
    "    \n",
    "example_sentences = [\n",
    "    \"This is an  example sentence\",\n",
    "    \"Another sentence\",\n",
    "    \"The final sentence\"\n",
    "]\n",
    "\n",
    "tokenized = pretokenize(example_sentences)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2d01b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T14:08:36.538134Z",
     "start_time": "2024-04-15T14:08:36.484836Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b80a983de1ea49369d28cb654b742017",
     "grade": true,
     "grade_id": "test_BytePairEncoding_A0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a73b7e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99c8c7a90be02c63999d2e2c554059c4",
     "grade": false,
     "grade_id": "BytePairEncoding_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding B) [10 points]\n",
    "\n",
    "For BPE we first need an initial vocabulary. The input is a pretokenized list of sentences / documents.\n",
    "\n",
    "The output should be a set of characters present in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaea461c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.481246Z",
     "start_time": "2024-04-20T00:37:44.420857Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "938c9e66990c5132f836b8912de9e3c5",
     "grade": false,
     "grade_id": "BytePairEncoding_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T', 'a', 'd', 'e', 'h', 'i', 'l', 'o', 'r', 's', 't', 'w'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "def build_initial_vocabulary(corpus: List[List[str]]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Build the initial vocabulary from a corpus of tokenized sentences.\n",
    "\n",
    "    Args:\n",
    "        corpus (List[List[str]]): A list of tokenized sentences, where each sentence\n",
    "            is represented as a list of strings (tokens).\n",
    "\n",
    "    Returns:\n",
    "        Set[str]: A set containing all unique tokens in the corpus.\n",
    "\n",
    "    Example:\n",
    "        >>> corpus = [['hello', 'world'], ['This', 'is', 'a', 'test']]\n",
    "        >>> build_initial_vocabulary(corpus)\n",
    "        {'T', 'a', 'd', 'e', 'h', 'i', 'l', 'o', 'r', 's', 't', 'w'}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    vocabulary = set() \n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            for letter in word:\n",
    "                vocabulary.add(letter)\n",
    "    return vocabulary\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "build_initial_vocabulary(pretokenize([\"hello world\", \"This is a test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058738b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9c4d770a17894a922c0f23e9a3e9030",
     "grade": true,
     "grade_id": "test_BytePairEncoding_B0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab5b925",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0644bf14c439654cc79b9717eeb3c62",
     "grade": false,
     "grade_id": "BytePairEncoding_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding C) [10 points]\n",
    "\n",
    "\n",
    "Now we want to build our dictionary for the split tokens. Complete the function `get_splits` below. Look at the example in the docstring!\n",
    "\n",
    "Make sure to add the end of word symbol (`</w>`) to each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f5e464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.549338Z",
     "start_time": "2024-04-20T00:37:44.486731Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "501a30353eeb5c4977819569515bf016",
     "grade": false,
     "grade_id": "BytePairEncoding_C",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
       " ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def get_splits(corpus: List[List[str]]) -> Dict[Tuple[str], int]:\n",
    "    \"\"\"\n",
    "    Get subword splits of tokens in a corpus.\n",
    "\n",
    "    Args:\n",
    "        corpus (List[List[str]]): A list of sentences where each sentence is represented\n",
    "            as a list of tokens.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str], int]: A dictionary where keys are tuples representing subword splits\n",
    "            and values are the counts of occurrences of those splits in the corpus.\n",
    "\n",
    "    Example:\n",
    "        >>> corpus = [['apple', 'banana', 'apple'], ['apple']]\n",
    "        >>> get_splits(corpus)\n",
    "        {('a', 'p', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    elements = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            elements.append(word)\n",
    "            \n",
    "    elements_counts = dict(Counter(elements))\n",
    "    \n",
    "    words_count = dict()\n",
    "    word_letters = []\n",
    "    for key in elements_counts.keys():\n",
    "        word_letters = [l for l in key]\n",
    "        word_letters.append('</w>')\n",
    "        word_letters = tuple(word_letters)\n",
    "        words_count[word_letters] = elements_counts.get(key)\n",
    "    \n",
    "    return words_count\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "get_splits(pretokenize([\"apple banana apple\", \"apple\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c2643",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "368952d06bdb7546506c7d8b7b2a5452",
     "grade": true,
     "grade_id": "test_BytePairEncoding_C0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c86d3f7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d25120cd363fa7b55da4974e989d2cc4",
     "grade": false,
     "grade_id": "BytePairEncoding_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding D) [10 points]\n",
    "\n",
    "In the next step we want to find the most common pair from a splits dictionary.\n",
    "\n",
    "Complete the function `find_most_frequent_pair` which returns the most frequent pair alongside its count (e.g. `(('a', 'n'), 2)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae731b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.644847Z",
     "start_time": "2024-04-20T00:37:44.577437Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0749785264a29f9db74f3a1e3ac5477",
     "grade": false,
     "grade_id": "BytePairEncoding_D",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('a', 'p'), 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_most_frequent_pair(splits: Dict[Tuple[str], int]) -> Tuple[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Find the most frequent pair of characters from a dictionary of split words along with its count.\n",
    "\n",
    "    Args:\n",
    "        splits (Dict[Tuple[str], int]): A dictionary where keys are tuples of split words and values are their counts.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tuple[str, str], int]: A tuple containing the most frequent pair of characters and its count.\n",
    "\n",
    "    Example:\n",
    "        >>> splits = {('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
    "                      ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n",
    "        >>> find_most_frequent_pair(splits)\n",
    "        (('a', 'n'), 2)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "#     voc = splits.keys()\n",
    "#     letters= []\n",
    "#     for tup in voc:\n",
    "#         lis = list(tup)\n",
    "#         for chara in lis:\n",
    "#             letters.append(chara)\n",
    "\n",
    "#     pair = []\n",
    "#     check = []\n",
    "#     count = 0\n",
    "#     pair_count = {}\n",
    "#     for ind, l in enumerate(letters):\n",
    "#         if ind < len(letters)-1:\n",
    "#             pair = [letters[ind],letters[ind +1]]\n",
    "#             for i, j in enumerate(letters):\n",
    "#                 if i < len(letters)-1:\n",
    "#                     check = [letters[i],letters[i +1]]\n",
    "#                     if check == pair:\n",
    "#                         count = count + 1\n",
    "#             t_pair = tuple(pair)\n",
    "#             pair_count[t_pair]= count\n",
    "#             pair.clear\n",
    "#             check.clear\n",
    "#             count = 0\n",
    "\n",
    "#     most_frequent_pair = max(pair_count, key=pair_count.get)\n",
    "#     frequency = pair_count[most_frequent_pair]\n",
    "\n",
    "#     return (most_frequent_pair,frequency)\n",
    "    voc = splits.keys()\n",
    "    words = []\n",
    "    letters= []\n",
    "    \n",
    "    for tup in voc:\n",
    "        lis = list(tup)\n",
    "        for _ in range(splits[tup]):\n",
    "            words.append(lis)\n",
    "    \n",
    "        \n",
    "    for word in words:\n",
    "        for letter in word:\n",
    "            letters.append(letter)\n",
    "    \n",
    "    pair = []\n",
    "    check = []\n",
    "    count = 0\n",
    "    pair_count = {}\n",
    "    for ind, l in enumerate(letters):\n",
    "        if ind < len(letters)-1:\n",
    "            pair = [letters[ind],letters[ind +1]]\n",
    "            for i, j in enumerate(letters):\n",
    "                if i < len(letters)-1:\n",
    "                    check = [letters[i],letters[i +1]]\n",
    "                    if check == pair:\n",
    "                        count = count + 1\n",
    "            t_pair = tuple(pair)\n",
    "            pair_count[t_pair]= count\n",
    "            pair.clear\n",
    "            check.clear\n",
    "            count = 0\n",
    "\n",
    "    most_frequent_pair = max(pair_count, key=pair_count.get)\n",
    "    frequency = pair_count[most_frequent_pair]\n",
    "\n",
    "    return (most_frequent_pair,frequency)\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "find_most_frequent_pair(get_splits(pretokenize([\"apple banana apple\", \"apple\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5958dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a8ddd02fa5547d8270eaba4c542572f",
     "grade": true,
     "grade_id": "test_BytePairEncoding_D0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae59af20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06c946410862f0b18d53883b2ba0930f",
     "grade": false,
     "grade_id": "BytePairEncoding_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding E) [15 points]\n",
    "\n",
    "Now write a function that takes a pair and the splits and merges all occurences of the pair in the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7016d5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.699469Z",
     "start_time": "2024-04-20T00:37:44.649142Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d0ef81953abcbb9abf94330664bc342",
     "grade": false,
     "grade_id": "BytePairEncoding_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('a', 'p', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n",
      "('a', 'p') 3\n",
      "{('ap', 'p', 'l', 'e', '</w>'): 3, ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "def merge_split(split: Tuple[str], pair: Tuple[str, str]):\n",
    "    \"\"\"\n",
    "    Merge a split tuple if it contains the given pair.\n",
    "\n",
    "    Args:\n",
    "        split (Tuple[str]): The split tuple to merge.\n",
    "        pair (Tuple[str, str]): The pair to merge.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str]: The merged split tuple.\n",
    "        \n",
    "    Example:\n",
    "        >>> merge_split(split=('a', 'b', 'c', 'b', 'c'), pair=('b', 'c'))\n",
    "        ('a', 'bc', 'bc')\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    list_split = list(split)\n",
    "    list_pair = list(pair)\n",
    "    new_split = []\n",
    "\n",
    "    for ind, letter in enumerate(list_split):\n",
    "        if ind < len(list_split)-1:\n",
    "            check = [list_split[ind],list_split[ind+1]]\n",
    "            if check == list_pair:\n",
    "                new_split.append(list_split[ind] + list_split[ind+1])\n",
    "            else:\n",
    "                if check[0] == list_pair[-1] and [list_split[ind-1], list_split[ind]] == list_pair:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_split.append(list_split[ind])\n",
    "        else:\n",
    "            check = [list_split[ind-1],list_split[ind]]\n",
    "            if check == list_pair:\n",
    "                continue\n",
    "            else:\n",
    "                if check[0] == list_pair[-1] and [list_split[ind-1], list_split[ind]] == list_pair:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_split.append(list_split[ind])\n",
    "                    \n",
    "    return tuple(new_split)\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def merge_splits(splits: Dict[Tuple[str], int], pair: Tuple[str, str]):\n",
    "    \"\"\"\n",
    "    Merge all split tuples in a dictionary that contain the given pair.\n",
    "\n",
    "    Args:\n",
    "        splits (Dict[Tuple[str], int]): A dictionary of split tuples and their counts.\n",
    "        pair (Tuple[str, str]): The pair to merge.\n",
    "\n",
    "    Returns:\n",
    "        Dict[Tuple[str], int]: A dictionary with merged split tuples and their counts.\n",
    "        \n",
    "    Example:\n",
    "        >>> merge_splits({('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
    "                          ('b', 'a', 'n', 'a', 'n', 'a', '</w>'): 1}, \n",
    "                          ('a', 'n'))\n",
    "        {('a', 'p', 'p', 'l', 'e', '</w>'): 3,\n",
    "         ('b', 'an', 'an', 'a', '</w>'): 1}\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    merged_split = {}\n",
    "    for key in splits.keys():\n",
    "        new_key = merge_split(key, pair)\n",
    "        merged_split[new_key] = splits[key]\n",
    "    return merged_split\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "splits = get_splits(pretokenize([\"apple banana apple\", \"apple\"]))\n",
    "print(splits)\n",
    "most_frequent_pair, count = find_most_frequent_pair(splits)\n",
    "print(most_frequent_pair, count)\n",
    "print(merge_splits(splits, most_frequent_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13299191",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbb68404c1e9e7653d07987d67aa40ff",
     "grade": true,
     "grade_id": "test_BytePairEncoding_E0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "063b44cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b399583c41ec36a39034590c5b122585",
     "grade": false,
     "grade_id": "BytePairEncoding_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding E) [40 points]\n",
    "\n",
    "Now let us put this all together into a single class. Complete the methods `train`, `encode` and `decode`.\n",
    "\n",
    "- `train` will learn the vocabulary and a list of merged pairs to use for encoding / tokenizing.\n",
    "- `encode` will tokenize a list of strings using the merge rules by applying them in order\n",
    "- `decode` will take a BPE encoded list of lists and merge subwords\n",
    "\n",
    "Look at the examples in the docstrings for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9684a22d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-20T00:37:44.787967Z",
     "start_time": "2024-04-20T00:37:44.706475Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c787ab0f4a5a98b4f250b388951253f1",
     "grade": false,
     "grade_id": "BytePairEncoding_F",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('lowest</w>',): 1, ('lower</w>',): 2, ('newe', 'r</w>'): 1, ('newe', 'st</w>'): 1, ('lo', 'w</w>'): 1, ('ne', 'w</w>'): 1}\n",
      "['l', 'o', 'w', 'e', 's', 't', '<', '/', 'w', '>', 'l', 'o', 'w', 'e', 'r', '<', '/', 'w', '>', 'n', 'e', 'w', 'e', 'r', '<', '/', 'w', '>', 'n', 'e', 'w', 'e', 's', 't', '<', '/', 'w', '>', 'l', 'o', 'w', '<', '/', 'w', '>', 'n', 'e', 'w', '<', '/', 'w', '>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lowest', 'lower', 'newer', 'newest', 'low', 'new']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    Byte-Pair Encoding (BPE) Tokenizer.\n",
    "    \n",
    "    This tokenizer learns a vocabulary and encodes/decodes text using the Byte-Pair Encoding algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BPETokenizer.\n",
    "        \"\"\"\n",
    "        self.vocab: set = set()\n",
    "        self.end_of_word: str = \"</w>\"\n",
    "        self.merge_pairs: List[Tuple[str, str]] = []\n",
    "        \n",
    "    def train(self, corpus: List[str], max_vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Train the tokenizer on a given corpus.\n",
    "        First pretokenizes the corpus using whitespace\n",
    "        Then uses BPE to update the vocabulary and learn the merge pairs\n",
    "\n",
    "        Args:\n",
    "            corpus (List[str]): The corpus of text for training.\n",
    "            max_vocab_size (int): The maximum size of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "        Example:\n",
    "        >>> corpus = [\n",
    "            \"lowest lower newer newest\",\n",
    "            \"low lower new\"\n",
    "        ]\n",
    "        >>> tokenizer.train(corpus, max_vocab_size=20)\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        self.tokenized = pretokenize(corpus)\n",
    "        self.vocab = build_initial_vocabulary(self.tokenized)\n",
    "        splits = get_splits(self.tokenized)\n",
    "#         print('splits =', splits)\n",
    "        \n",
    "        for i in range(max_vocab_size):\n",
    "                                \n",
    "            most_frequent_pair, count = find_most_frequent_pair(splits)\n",
    "            self.merge_pairs.append(most_frequent_pair)\n",
    "#             print('merge_pairs =',self.merge_pairs)\n",
    "\n",
    "            new_pair = ''.join(most_frequent_pair)\n",
    "            self.vocab.add(new_pair)\n",
    "#             print('vocab =', self.vocab)\n",
    "            \n",
    "            splits = merge_splits(splits, most_frequent_pair) #get_splits(self.tokenized)\n",
    "#             print('splits =', splits)\n",
    "            if len(self.vocab) == max_vocab_size:\n",
    "                break\n",
    "        \n",
    "    def encode(self, corpus: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Encode / Tokenize a corpus of text using the learned vocabulary and merge pairs.\n",
    "\n",
    "        Args:\n",
    "            corpus (List[str]): The corpus of text to be encoded.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: The encoded corpus.\n",
    "        \n",
    "        Example:\n",
    "        >>> corpus = [\n",
    "            \"lowest lower newer newest\",\n",
    "            \"low lower new\"\n",
    "        ]\n",
    "        >>> tokenizer.train(corpus, max_vocab_size=20)\n",
    "        >>> tokenizer.encode(corpus)\n",
    "        [['lowest</w>', 'lower</w>', 'newer</w>', 'newe', 'st</w>'],\n",
    "         ['lo', 'w</w>', 'lower</w>', 'ne', 'w</w>']]\n",
    "        \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        self.tokenized = pretokenize(corpus)\n",
    "        self.splits = get_splits(self.tokenized)\n",
    "        self.encodes = []\n",
    "        for pair in self.merge_pairs:\n",
    "            self.splits = merge_splits(self.splits, pair)\n",
    "        print(self.splits)\n",
    "        \n",
    "        for enc in self.splits.keys():\n",
    "            l_enc = list(enc)\n",
    "            self.encodes.append(l_enc)    \n",
    "        return self.encodes \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def decode(self, tokenized: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Decode a corpus of tokenized text.\n",
    "\n",
    "        Args:\n",
    "            tokenized (List[List[str]]): The tokenized text to be decoded.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: The decoded text.\n",
    "            \n",
    "        Example:\n",
    "        >>> corpus = [\n",
    "            \"lowest lower newer newest\",\n",
    "            \"low lower new\"\n",
    "        ]\n",
    "        >>> tokenizer.train(corpus, max_vocab_size=20)\n",
    "        >>> tokenizer.decode([['lowest</w>', 'lower</w>', 'newer</w>', 'newe', 'st</w>'],\n",
    "                              ['lo', 'w</w>', 'lower</w>', 'ne', 'w</w>']])\n",
    "        [['lowest', 'lower', 'newer', 'newest'], ['low', 'lower', 'new']]                              \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        letters = []\n",
    "        decode = []\n",
    "        self.decodes = []\n",
    "        for enc in tokenized:\n",
    "            for word in enc:\n",
    "                for letter in word:\n",
    "                    letters.append(letter)\n",
    "        print(letters)       \n",
    "        # letters = ''.join(letters)\n",
    "\n",
    "        for letter in letters:\n",
    "            if letter == '<':\n",
    "                ind = letters.index('<')\n",
    "                decode.append(letters[0:ind])\n",
    "        #         print(decode)\n",
    "                letters = letters[ind+4:]\n",
    "\n",
    "        for dec in decode:\n",
    "            w = ''.join(dec)\n",
    "            self.decodes.append(w)\n",
    "        return self.decodes\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "corpus = [\n",
    "    \"lowest lower newer newest\",\n",
    "    \"low lower new\"\n",
    "]\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(corpus, 20)\n",
    "# tokenizer.encode(corpus)\n",
    "tokenizer.decode(tokenizer.encode(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3bc54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0dda862511776b696c2274cfbcc32738",
     "grade": true,
     "grade_id": "test_BytePairEncoding_F0",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90a8b85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d08837f7e527668a717a09ec705eae53",
     "grade": false,
     "grade_id": "BytePairEncoding_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Byte Pair Encoding F) [5 points]\n",
    "\n",
    "Use your BPE tokenizer on the movie script of spider. Then encode a random sentence using the tokenizer. Finally decode the sentence again.\n",
    "\n",
    "Training might take ~3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de503a8e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-20T00:37:44.269Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c153effcad1b71f987204eeb8dafcbfd",
     "grade": true,
     "grade_id": "BytePairEncoding_G",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/srv/shares/NLP/datasets/yelp/reviews_sents.txt\", \"r\") as f:\n",
    "    sentences = f.read().split(\"\\n\")\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(sentences, 20)\n",
    "tokenizer.decode(tokenizer.encode(sentences))\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad390c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
